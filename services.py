import redis
import json
import hashlib
import asyncio
import aiohttp
from mistralai import Mistral
from config import config
from database import Session, Place
from bs4 import BeautifulSoup
import re
from datetime import datetime
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CacheService:
    def __init__(self):
        try:
            self.redis = redis.Redis(
                host=config.REDIS_HOST,
                port=config.REDIS_PORT,
                decode_responses=True
            )
            self.redis.ping()
            logger.info("‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
        except redis.ConnectionError:
            logger.warning("‚ö†Ô∏è Redis –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω. –ö—ç—à –æ—Ç–∫–ª—é—á–µ–Ω.")
            self.redis = None
    
    def get_cache_key(self, prefix: str, query: str) -> str:
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return f"{prefix}:{query_hash}"
    
    def get(self, key: str):
        if not self.redis:
            return None
        data = self.redis.get(key)
        return json.loads(data) if data else None
    
    def set(self, key: str, data, ttl: int = 300):
        if self.redis:
            self.redis.setex(key, ttl, json.dumps(data))
    
    def set_url_content(self, url: str, content: str, ttl: int = 3600):
        """–ö—ç—à–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if self.redis:
            key = f"url:{hashlib.md5(url.encode()).hexdigest()}"
            self.redis.setex(key, ttl, content)
    
    def get_url_content(self, url: str):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∫—ç—à–∞"""
        if not self.redis:
            return None
        key = f"url:{hashlib.md5(url.encode()).hexdigest()}"
        return self.redis.get(key)

class WebParser:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –≤–µ–±-—Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
        }
    
    async def __aenter__(self):
        # –ò–ì–ù–û–†–ò–†–£–ï–ú SSL –û–®–ò–ë–ö–ò
        connector = aiohttp.TCPConnector(ssl=False)
        self.session = aiohttp.ClientSession(
            connector=connector,
            headers=self.headers
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_url(self, url: str) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            async with self.session.get(url, timeout=10, ssl=False) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return ""
    
    def parse_page_content(self, html: str, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            if not html:
                return {"url": url, "content": "", "title": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏"}
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_main_content(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã
            contacts = self._extract_contacts(content)
            
            # –ß–∏—Å—Ç–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
            clean_content = self._clean_text(content)
            
            return {
                "url": url,
                "title": title,
                "content": clean_content[:3000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä–µ–º
                "contacts": contacts,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            return {"url": url, "content": "", "title": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", "error": str(e)}
    
    def _extract_title(self, soup, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = [
            'h1',
            'title',
            'meta[property="og:title"]',
            'meta[name="twitter:title"]',
            '.title',
            '.page-title',
            '.article-title'
        ]
        
        for selector in selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith('meta'):
                        title = element.get('content', '').strip()
                    else:
                        title = element.get_text().strip()
                    
                    if title and len(title) > 3:
                        return title
            except:
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –∏–∑ URL
        return url.split('//')[-1].split('/')[0].replace('www.', '').capitalize()
    
    def _extract_main_content(self, soup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        content_selectors = [
            'main',
            'article',
            '.content',
            '.article-content',
            '.post-content',
            '#content',
            '.text',
            '.description'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                texts = [elem.get_text().strip() for elem in elements]
                combined = ' '.join(texts)
                if len(combined) > 100:
                    return combined
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –±–µ—Ä–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = soup.find_all('p')
        texts = [p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 20]
        return ' '.join(texts[:15])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    
    def _extract_contacts(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        contacts = {
            "address": self._find_pattern(text, r'—É–ª\.?\s+[\w\s\d\-]+,\s*\d+|[–ê-–Ø–∞-—è][^,\n]{10,50},\s*\d+'),
            "phone": self._find_pattern(text, r'\+7\s?\(?\d{3}\)?\s?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}|8\s?\(?\d{3}\)?\s?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}'),
            "email": self._find_pattern(text, r'[\w\.-]+@[\w\.-]+\.\w+'),
            "hours": self._find_pattern(text, r'[–ü–ø]–Ω\.?-[–í–≤]—Å\.?\s*\d{1,2}:\d{2}-\d{1,2}:\d{2}|\d{1,2}:\d{2}\s*[-‚Äì]\s*\d{1,2}:\d{2}')
        }
        
        return {k: v if v else "–ù–µ —É–∫–∞–∑–∞–Ω–æ" for k, v in contacts.items()}
    
    def _find_pattern(self, text: str, pattern: str) -> str:
        """–ò—â–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –≤ —Ç–µ–∫—Å—Ç–µ"""
        match = re.search(pattern, text)
        return match.group() if match else ""
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'[^\w\s.,!?;:()-]', '', text)
        return text.strip()

class LLMService:
    def __init__(self):
        self.client = Mistral(api_key=config.MISTRAL_API_KEY)
        self.cache = CacheService()
        self.url_database = config.URL_DATABASE
    
    def analyze_preferences(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        cache_key = self.cache.get_cache_key("pref", text)
        
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # –£–°–ò–õ–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ –î–õ–Ø JSON
        prompt = f"""–¢—ã –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –¢–û–õ–¨–ö–û JSON –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{text}"

–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–µ—Å—Ç –æ—Ç–¥—ã—Ö–∞ –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞: {', '.join(config.CATEGORIES)}

–¢—ã –î–û–õ–ñ–ï–ù –≤–µ—Ä–Ω—É—Ç—å –¢–û–õ–¨–ö–û JSON –≤ —Ç–æ—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ:
{{
    "categories": ["–∫–∞—Ç–µ–≥–æ—Ä–∏—è1", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è2"],
    "explanation": "–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞"
}}

–í–ê–ñ–ù–û: –¢–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤.
"""
        
        try:
            response = self.client.chat.complete(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": "–¢—ã –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON. –ù–∏–∫–∞–∫–æ–≥–æ –¥—Ä—É–≥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏
            if response_text.startswith('```'):
                response_text = response_text.split('```')[1]
                if response_text.startswith('json'):
                    response_text = response_text[4:]
                response_text = response_text.strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            result = json.loads(response_text)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            if "categories" not in result:
                result["categories"] = []
            if "explanation" not in result:
                result["explanation"] = "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
            
            self.cache.set(cache_key, result, ttl=1800)
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback
            return self._fallback_category_detection(text)
    
    def _fallback_category_detection(self, text: str) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥)"""
        text_lower = text.lower()
        categories = []
        
        keyword_mapping = {
            "üèõÔ∏è –ú—É–∑–µ–∏": ['–º—É–∑–µ–π', '–º—É–∑–µ–∏', '–∏—Å–∫—É—Å—Å—Ç–≤', '–∏—Å—Ç–æ—Ä–∏', '—ç–∫—Å–ø–æ–∑', '–∫–æ–ª–ª–µ–∫—Ü'],
            "üé® –ò—Å–∫—É—Å—Å—Ç–≤–æ/–í—ã—Å—Ç–∞–≤–∫–∏": ['–∏—Å–∫—É—Å—Å—Ç–≤', '–≤—ã—Å—Ç–∞–≤–∫', '–≥–∞–ª–µ—Ä–µ', '—Ö—É–¥–æ–∂', '–∞—Ä—Ç', '–∫–∞—Ä—Ç–∏–Ω'],
            "üçΩÔ∏è –†–µ—Å—Ç–æ—Ä–∞–Ω—ã/–ö–∞—Ñ–µ": ['—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ', '–µ–¥–∞', '–∫—É—Ö–Ω', '–æ–±–µ–¥', '—É–∂–∏–Ω', '–∑–∞–≤—Ç—Ä–∞–∫'],
            "‚òï –ö–æ—Ñ–µ–π–Ω–∏": ['–∫–æ—Ñ–µ', '–∫–æ—Ñ–µ–π–Ω', '–ª–∞—Ç—Ç–µ', '–∫–∞–ø—É—á–∏–Ω', '—ç—Å–ø—Ä–µ—Å—Å–æ'],
            "üèûÔ∏è –ü–∞—Ä–∫–∏/–ü—Ä–æ–≥—É–ª–∫–∏": ['–ø–∞—Ä–∫', '–ø—Ä–æ–≥—É–ª', '—Å–∫–≤–µ—Ä', '–∞–ª–ª–µ', '–æ—Ç–¥—ã—Ö', '–ø—Ä–∏—Ä–æ–¥'],
            "üé≠ –¢–µ–∞—Ç—Ä—ã/–ö–æ–Ω—Ü–µ—Ä—Ç—ã": ['—Ç–µ–∞—Ç—Ä', '–∫–æ–Ω—Ü–µ—Ä—Ç', '—Å–ø–µ–∫—Ç–∞–∫–ª', '–æ–ø–µ—Ä', '–±–∞–ª–µ—Ç'],
            "üé≥ –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è": ['–∫–∏–Ω–æ', '–±–æ—É–ª–∏–Ω–≥', '–∫–≤–µ—Å—Ç', '–∞—Ç—Ç—Ä–∞–∫—Ü–∏–æ–Ω', '—Ä–∞–∑–≤–ª–µ—á–µ–Ω'],
            "üõçÔ∏è –®–æ–ø–ø–∏–Ω–≥": ['–º–∞–≥–∞–∑–∏–Ω', '—à–æ–ø–ø–∏–Ω–≥', '—Ç–æ—Ä–≥–æ–≤', '–ø–æ–∫—É–ø', '–±—É—Ç–∏–∫'],
            "üé™ –°–æ–±—ã—Ç–∏—è/–§–µ—Å—Ç–∏–≤–∞–ª–∏": ['—Ñ–µ—Å—Ç–∏–≤–∞–ª', '—Å–æ–±—ã—Ç–∏', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç', '–ø—Ä–∞–∑–¥–Ω–∏–∫'],
            "üçª –ë–∞—Ä—ã/–ü–∞–±—ã": ['–±–∞—Ä', '–ø–∞–±', '–ø–∏–≤–æ', '–∫–æ–∫—Ç–µ–π–ª', '–Ω–∞–ø–∏—Ç–æ–∫']
        }
        
        for category, keywords in keyword_mapping.items():
            if any(keyword in text_lower for keyword in keywords):
                categories.append(category)
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if not categories:
            categories = ["üèõÔ∏è –ú—É–∑–µ–∏", "üçΩÔ∏è –†–µ—Å—Ç–æ—Ä–∞–Ω—ã/–ö–∞—Ñ–µ", "üèûÔ∏è –ü–∞—Ä–∫–∏/–ü—Ä–æ–≥—É–ª–∫–∏"]
        
        return {
            "categories": categories[:3],  # –ù–µ –±–æ–ª—å—à–µ 3 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            "explanation": f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {', '.join(categories[:3])}"
        }
    
    async def get_recommendations(self, categories: List[str]) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        cache_key = self.cache.get_cache_key("rec", str(categories))
        
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # 1. –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        urls_to_parse = []
        for category in categories:
            if category in self.url_database:
                urls_to_parse.extend(self.url_database[category][:3])  # –ë–µ—Ä–µ–º –ø–æ 3 —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        
        if not urls_to_parse:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        
        # 2. –ü–∞—Ä—Å–∏–º —Å–∞–π—Ç—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        parsed_data = await self._parse_urls_async(urls_to_parse)
        
        # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        recommendations = await self._generate_summary(parsed_data, categories)
        
        # 4. –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache.set(cache_key, recommendations, ttl=3600)
        
        return recommendations
    
    async def _parse_urls_async(self, urls: List[str]) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ URL"""
        parsed_results = []
        
        async with WebParser() as parser:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [parser.fetch_url(url) for url in urls]
            html_contents = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –ü–∞—Ä—Å–∏–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            for url, html in zip(urls, html_contents):
                if isinstance(html, Exception):
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {html}")
                    parsed_results.append({
                        "url": url,
                        "content": "",
                        "title": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏",
                        "error": str(html)
                    })
                else:
                    parsed = parser.parse_page_content(html, url)
                    parsed_results.append(parsed)
                    
                    # –ö—ç—à–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    if parsed["content"]:
                        self.cache.set_url_content(url, parsed["content"])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful = [p for p in parsed_results if p.get("content") and len(p["content"]) > 50]
        
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(successful)} –∏–∑ {len(urls)} —Å–∞–π—Ç–æ–≤")
        return successful
    
    async def _generate_summary(self, parsed_data: List[Dict[str, Any]], categories: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not parsed_data:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å —Å–∞–π—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        sites_info = []
        for data in parsed_data:
            site_info = f"""
            –°–∞–π—Ç: {data['url']}
            –ù–∞–∑–≤–∞–Ω–∏–µ: {data['title']}
            –ö–æ–Ω—Ç–µ–Ω—Ç: {data['content'][:800]}...
            –ö–æ–Ω—Ç–∞–∫—Ç—ã: {json.dumps(data.get('contacts', {}), ensure_ascii=False)}
            """
            sites_info.append(site_info)
        
        separator = "=" * 50
        prompt = f"""
        –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—â–µ—Ç –º–µ—Å—Ç–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö: {', '.join(categories)}
        
        –Ø –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª —Å–ª–µ–¥—É—é—â–∏–µ —Å–∞–π—Ç—ã:
        {separator}
        {separator.join(sites_info)}
        {separator}
        
        –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤—å –ö–†–ê–¢–ö–ò–ï —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏(–¥–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤):
        1. –°–Ω–∞—á–∞–ª–∞ –∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä: –∫–∞–∫–∏–µ –º–µ—Å—Ç–∞ –Ω–∞—à–ª–∏—Å—å, –∫–∞–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ
        2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Å—Ç–∞ (—Å–∞–π—Ç–∞) –¥–∞–π: 
           - –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
           - –ü–æ—á–µ–º—É —Å—Ç–æ–∏—Ç –ø–æ—Å–µ—Ç–∏—Ç—å (2-3 –ø—É–Ω–∫—Ç–∞)
           - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–∞–¥—Ä–µ—Å –µ—Å–ª–∏ –µ—Å—Ç—å, –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å)
        3. –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: –∫–æ–≥–¥–∞ –ª—É—á—à–µ –ø–æ—Å–µ—â–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å
        4. –í –∫–æ–Ω—Ü–µ: "–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–∞–π—Ç–æ–≤: [—Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤]"
        
        –ë—É–¥—å –ö–†–ê–¢–ö–ò–ú, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –ø–æ–ª–µ–∑–Ω—ã–º –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º. –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ —Å —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.
        –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–∞–ª–æ - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.
        """
        
        try:
            response = self.client.chat.complete(
                model=config.LLM_MODEL,
                messages=[
                    {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å —Å–∞–π—Ç–æ–≤ –∏ –¥–∞–µ—à—å –ö–†–ê–¢–ö–ò–ï —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–µ—Å—Ç–∞–º –æ—Ç–¥—ã—Ö–∞. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def get_available_categories(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏"""
        return list(self.url_database.keys())

class AdminService:
    @staticmethod
    def add_url_to_category(category: str, url: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Å—ã–ª–∫—É –≤ –±–∞–∑—É (–¥–ª—è –∞–¥–º–∏–Ω–∞)"""
        if category in config.URL_DATABASE:
            if url not in config.URL_DATABASE[category]:
                config.URL_DATABASE[category].append(url)
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é {category}: {url}")
                return True
        return False
    
    @staticmethod
    def get_url_stats() -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º"""
        stats = {}
        for category, urls in config.URL_DATABASE.items():
            stats[category] = len(urls)
        return stats